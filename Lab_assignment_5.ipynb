{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>ARTIFICIAL INTELLIGENCE (E016330)</b> <br>\n",
    "ALEKSANDRA PIZURICA <br>\n",
    "GHENT UNIVERSITY <br>\n",
    "AY 2022/2023 <br>\n",
    "Assistent: Srdan Lazendic\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment 5: CNNs and Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will first train a CNN for the classification problem and then an autoencoderencoder that learns representations of images of the `fashion_mnist` dataset. Finally, you will create a simple image retrieval system by using the trained autoencoder and the nearest-neighbors algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset `Fashion MNIST`\n",
    "\n",
    "This examples uses [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images split into 10 categories. Each image contains one peace of clothing or footwear. \n",
    "Resolution of each image is $28 \\times 28$ pixels.\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Image 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST examples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST is intended as a replacement for the traditional [MNIST](http://yann.lecun.com/exdb/mnist/) dataset that was previously often used as the first illustrative data set for classification (with images). `MNIST` contains images of handwritten numbers in a very similar format as previously described for `Fashion MNIST`.\n",
    "\n",
    "Fashion `MNIST` is a somewhat heavier set of data compared to `MNIST`, but both sets are extremely clean (no noise, deviations in the corners of the photo, relatively similar lighting), and solving the classification problem on them is not difficult.\n",
    "\n",
    "In this example, we will use $40000$ images to train the network, $20000$ images for the validation and the other $10000$ images to evaluate how accurately our network has classified the images.\n",
    "\n",
    "Since `Fashion MNIST` is a relatively well-known set that is often used, it is not uncommon for libraries to provide auxiliary functions for its download and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation, Dense, Input, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Using the `load_data` function from the `fashion_mnist` package, load images for the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data set \n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Select the first $40000$ images from the loaded set for training, and then the next $20000$ for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[40000:60000]\n",
    "y_val = y_train[40000:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:40000]\n",
    "y_train = y_train[:40000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Transform the loaded images into the `float32` type, and then normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the images to float32 type\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "x_train /= 255\n",
    "x_val /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Create a convolutional neural network with the following properties:\n",
    "\n",
    "- Sequential model consisting of convolutional and max pooling layers used alternately.\n",
    "- Three convolutional layers consist of  $32$, $64$ and $128$  kernels, respectively and they are all of size $3\\times 3$, with the  $\\mathrm{ReLU}$  activation function.\n",
    "- Each of the two max pooling layers is with the pool size  $2\\times 2$.\n",
    "- After the third convolutional layer use the `Flatten` layer.\n",
    "- Finally, add on top two fully connected layers with $128$ and $10$  neurons, with $\\mathrm{ReLU}$ and *softmax* activation functions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 13:52:47.586977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/seoyangsam/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-12-21 13:52:47.588252: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-21 13:52:47.588355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-21 13:52:47.591879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Train the previously defined network in $10$ epochs with the batch size equal to $32$. For the error, use `categorical cross entropy`, other parameters you may choose arbitrarily. Create a textual summary of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_cat = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "val_labels_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "test_labels_cat = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 23s 17ms/step - loss: 0.6267 - accuracy: 0.7706 - val_loss: 0.4555 - val_accuracy: 0.8327\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.4181 - accuracy: 0.8464 - val_loss: 0.4070 - val_accuracy: 0.8498\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.3538 - accuracy: 0.8710 - val_loss: 0.3706 - val_accuracy: 0.8637\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.3129 - accuracy: 0.8844 - val_loss: 0.3316 - val_accuracy: 0.8799\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.2793 - accuracy: 0.8965 - val_loss: 0.3293 - val_accuracy: 0.8803\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.2520 - accuracy: 0.9062 - val_loss: 0.3213 - val_accuracy: 0.8846\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.2317 - accuracy: 0.9128 - val_loss: 0.3112 - val_accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.2131 - accuracy: 0.9208 - val_loss: 0.2963 - val_accuracy: 0.8939\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.1906 - accuracy: 0.9292 - val_loss: 0.3180 - val_accuracy: 0.8893\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.1737 - accuracy: 0.9341 - val_loss: 0.3101 - val_accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, train_labels_cat, epochs = epochs,\n",
    "                    batch_size = batch_size,  validation_data = (x_val, val_labels_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1, 1, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,474\n",
      "Trainable params: 110,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a textual summary of our model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** \n",
    "- Plot the accuracy on the training and validation datasets over training epochs. Add a legend.\n",
    "- What do you observe from the obtained plot?\n",
    "- How could you improve the previous model? Explain shortly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 0.3545 - accuracy: 0.8875 - 2s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzde1yUZf7/8fcwwAxnFOSsiKml4iHRPNtp11arzerbudSs3dy2dc3s4K+t3dxa0r6alatliZa5rVtW63b6xlZbppWKYrmilqIcBBFEQM7MzO+PgdERVAaBgfH1fDzux8x9z33f8xkg5911Xfd1G2w2m00AAAAewsvdBQAAALQmwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKG4NN1999ZWuvfZaxcTEyGAw6P333z/rMV9++aWSkpJkNpvVq1cvvfzyy+1QKQAA6CzcGm7Ky8s1ePBgLVmypFn7Z2ZmatKkSRo3bpy2b9+u//f//p9mzpypdevWtXGlAACgszB0lBtnGgwGvffee5o8efJp93n00Ue1fv16ZWRkOLbNmDFDO3bs0DfffNMeZQIAgA7O290FuOKbb77RhAkTnLZdddVVWrFihWpra+Xj49PomOrqalVXVzvWrVarjh49qrCwMBkMhjavGQAAnDubzaaysjLFxMTIy+vMHU+dKtzk5+crMjLSaVtkZKTq6upUWFio6OjoRsckJyfrqaeeaq8SAQBAG8rOzlZcXNwZ9+lU4UZSo9aWhl6107XCzJ07V7Nnz3asl5SUqEePHsrOzlZwcHDbFQoAAFpNaWmpunfvrqCgoLPu26nCTVRUlPLz8522FRQUyNvbW2FhYU0eYzKZZDKZGm0PDg4m3AAA0Mk0Z0hJp5rnZtSoUUpNTXXa9umnn2rYsGFNjrcBAADnH7eGm+PHjys9PV3p6emS7Jd6p6enKysrS5K9S2nKlCmO/WfMmKGDBw9q9uzZysjIUEpKilasWKE5c+a4pX4AANDxuLVbauvWrbr88ssd6w1jY6ZOnapVq1YpLy/PEXQkKSEhQR999JEefPBB/fWvf1VMTIxefPFF3Xjjje1eOwAA6Jg6zDw37aW0tFQhISEqKSlhzA0AAJ2EK9/fnWrMDQAAwNkQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKIQbAADgUQg3AADAoxBuAACARyHcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAA8CiEGwAA4FEINwAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKIQbAADgUbzdXQAAAOjcbDabyqrrdKy8Vscqa1RrsSkpvovb6iHcAAAASfaQUllr0bGKWhVX1KikolbFDc8ra1VcXqNjlbU6VlGj4gr747GKWh2rrJXFanOcJzbUTxsfu8Jtn4NwAwCAB6qps9rDR5OhpOF5fTipsLe4FFfUqqbO2uL3NPt4qYu/ryKDTa34SVxHuAEAoAOzWG32VhNHEDm11eSU9frn5TWWFr+nt5dBof6+6uLvoy7+vgrx91EXfx+F+vsqtH5bqN8p6/4+MvsYW/GTtxzhBgCAdmaz2VRaWafs4grlHqtUTnGlcosrVVRe7QgnDS0upVV1LX4fL4MUcmoIqV+3h5XGASXU31cBvkYZDIZW/MTti3ADAEArs9lsOlZRq5ziSuWcFGByiiscQaas2rXQEmTyVmhAfUuKn3MYcQoq9a918fdVkNlbXl6dN6S0FOEGAAAX2Ww2HS2vqQ8slco9VuF4nlNcodziymZ1C4UH+iq2i7/iQv0U28VP3QJNpwQWe4AJ8fORj5HZW5qLcAMAwClsNpuOHK9WriOwOAeY3OJKVdaePbx0CzIprouf4rr4KzbUr/65fYkN9Zefb8cYo+JpCDcAgPOO1WoPLw3dRCcCzImWl+qzXDVkMEgRQSbFdfF3CiwNz2NC/TrMANvzDeEGAOBxLFabCsqqHK0sJ4eY3GP2bTWWM4cXL4MUFWy2t7qc0uIS18VP0aFmmbwJLx0R4QYA0OnU1Fl1uLTKEVQaWlwaAkxeSaVqLbYznsPoZagPL36NAkxcqL+iQszy9WacS2dEuAEAdCg2m31el4bgcuhYpQ6V2IPMofqloKxatjNnF3l7GRQdalaco6vIOcBEBZvlzSBdj0S4AQC0q1NbXezhpVK5x6oc4aWiGVcamby9FBvq5xxgup7oNooMNst4Hl4GDcINAKAVndzqcuhYlXKLK1rU6iJJ4YEmxYaaFRPq51hiQ82KDfVXTKhZXQN8O/VEc2g7hBsAQLOd3OpyqKHl5RxaXeyhxXxSeLE/RoeYudIILUa4AQBIatzqcuhY/ZVFrdjq0vA8jFYXtCHCDQCcB6xWm4rKa1RQVqWCsmodKa3W4dKqRl1GzWl18a1vdYml1QUdFOEGADqxWotVR8qqVVBWrYJSe3ApKKvWkbIqFZTWby+rUuHxGlmszWhykf2WACeHFVpd0NkQbgCgA6qssThaWewhxfl5Q6A5Wl7T7HMaDFJYgEkRQSZFBNsfaXWBJyLcAEA7sdlsKq2qa9Sq4vS8vsvIlTtG+xgN6hZoUrdgsz24BJkUEWR2BJiG52EBvszrgvMC4QYAzpHValNxRY2jS8jRPXRSN1FDa0tV7Zmn/D+Zn4/RKaB0c7S4mE9qfTEr1M9HXsznAjgQbgDgLKpqLTpYVKHMwnIdLCpX1tEKHS6tH9dSVq0jZdWqa+Z4FkkKMnuftnWlW5BJkfUtMIEmb8a2AC1AuAEA2QNM1tETASazsEIHCst1oKhceSVVzTpHWIBvfevKyd1DJ6/bAwxjWoC25fZws3TpUj333HPKy8vTgAEDtHjxYo0bN+60+//1r3/VkiVLdODAAfXo0UOPP/64pkyZ0o4VA+isqmotyj5aoQNF9uCSWVSuA4XlOlhUoUMllWecvyXY7K2E8AD1DA9QfFd/RYaYnbqHwgNN8mE8C9AhuDXcrF27VrNmzdLSpUs1ZswYvfLKK5o4caJ27dqlHj16NNp/2bJlmjt3rl599VUNHz5cmzdv1q9+9St16dJF1157rRs+AYCOprquPsAUVuhAUbky61tfDhSePcAE1QeY+LAAJYT5q2d9mOkZFqAu/j50EQGdhMFma85ck21jxIgRGjp0qJYtW+bY1q9fP02ePFnJycmN9h89erTGjBmj5557zrFt1qxZ2rp1q77++utmvWdpaalCQkJUUlKi4ODgc/8QANpdTZ1V2cX1rS/14aVhTMyhY5U60/CXIJO3vfUlzN/eEhMWoJ7h/uoZFsC9ioAOzJXvb7e13NTU1CgtLU2PPfaY0/YJEyZo06ZNTR5TXV0ts9nstM3Pz0+bN29WbW2tfHx8mjymurrasV5aWtoK1QNoazV1VuUUN7S+nBj/cqCoXLnFZw4wAb5GR6tLQlhD64u9JYYJ6ADP57ZwU1hYKIvFosjISKftkZGRys/Pb/KYq666Sq+99pomT56soUOHKi0tTSkpKaqtrVVhYaGio6MbHZOcnKynnnqqTT4DgHNTa7Eqp7jS0QJzsKhcmfXjYXKPVZ5xRl1/X6N6hgXUj4Pxt3cl1bfEhAcSYIDzmdsHFJ/6D5DNZjvtP0pPPPGE8vPzNXLkSNlsNkVGRmratGlasGCBjMamrz6YO3euZs+e7VgvLS1V9+7dW+8DADirouPV2pNfpr2Hy3SgvvvoQFG5corPHmDsocXebdSzoRUm3F/dAk0EGABNclu4CQ8Pl9FobNRKU1BQ0Kg1p4Gfn59SUlL0yiuv6PDhw4qOjtby5csVFBSk8PDwJo8xmUwymUytXj+AxqrrLNpXUK7d+aXanV+mjDz745Gy6tMe4+djdIx/OTnIJIQHqFsQAQbo0Gw2qaZcKj8ilRfWPx6x3+tjqPuuZHZbuPH19VVSUpJSU1N1/fXXO7anpqbquuuuO+OxPj4+iouLkyT9/e9/1zXXXCMvLy7BBNqLzWZTXkmVI8TszivT7vxS7T9S3uRkdgaDFN/VX30jg5TQ7eRxMAGKDCbAeDSbTbJaJGutZKmVrHX1j6dZP+1rdfWPNY1fs1ok3wDJHCr5dZH8Quuf1z/6+Nn/CNE8dTVSxUlBpfxMzwulusrG5wiKOT/DjSTNnj1bd911l4YNG6ZRo0Zp+fLlysrK0owZMyTZu5Ryc3P1xhtvSJL27t2rzZs3a8SIESouLtaiRYu0c+dOvf766+78GIBHK6+u057D9gCzJ79UGfll2p1XqtKqpu99FOLno4uigtQvOlgXRQXpwqgg9Y0MUoDJ7b3gqK2SSnOlkhz7UnbIvs0pPDQVNOpOHzgsNac5/qR1ue2iXDujqXHgaSoENRmMzGc/f0dntUqVxSeFklNDSv16Q6CpKnH9PbzNUkCEFBAuBXSTghuPgW1Pbv3X5pZbblFRUZHmzZunvLw8JSYm6qOPPlJ8fLwkKS8vT1lZWY79LRaLFi5cqD179sjHx0eXX365Nm3apJ49e7rpEwCew2K1KetohXbnnQgwew6X6WBRRZP7e3sZdEG3QF0UHaSLooLrH4MUFWymJcYdbDb7F1RJ9onwUpLjvF5e4O4qTzD6Sl4+ktG7/tGniXVv+6PR98Tzpo7x8rJ3jVQek6qO2R8ri+1f0jaLZKmWjh+2L67y9mt5MPL2bf2fm1TfFXT89CHF8bzI/lhRKNmaf08zSZLBeCKoOB5Pfd5N8g+zP/oGdKjWMbfOc+MOzHMDSMXlNfbupPxSR5fS3sPHVVlraXL/iCCTLooOVr+oIEeY6dUtQCZvbiPQbmorpZLcM4cXy+nHNjn4+Esh3aWQOPv/XfsEOAeJMwaM07zmtO57htcawoixfb4IbTapuuyUwHPMOQRV1W9vtO2YzrnFySegiRBUH4Sa3BZSP37lDIGloj6w1DXvliBO/LpI/s0ILAHh9ro62HCPTjHPDYC2V1Nn1b4jx7Unv0wZJwWZw6VNfwmafbzUN9LeAnOiNSZYXQPa6P9AYWe12r+wTg0rJz+vKGzGiQxSUNSJ8BISd8rzOPsXXAf6P+w2ZTBI5mD7Etp41vszslql6tLGgac5AamqVJJNqi23L6W5bfLx5BNwSutKEyHl5BYWY+O54DwV4QbwADabTYdLq5WRX6o99V1Ku/PL9FPB8dPerbpHV39dGBVU3xpjHx8THxYgo9d58sXXnmrKz9zqUpprH7tyNj4BUugpYeXk8BIU03ZdIecbL6/6FpVQqYuLx1ot9mB0uhahpgJS5TF7N5qv/ymh5HShJdzeFYQmEW6ATqaipk57Dx93BJiGK5aOVdQ2uX+Q2Vv9ooJ14UldShdGBSmQAb6tw2q1j+Vo1Opy0nrl0bOfx+AlBUWfPriExNm7Cs6XVpfOzMtYPwbH1VSE1sK/bkAHZbPZlF9apR9ySrQrz96ltOdwmQ4UlTd580ejl0G9wgMcrTAX1bfIxIQwwLfZrFappsz+f9AnLw3/V+1YP3qiJab0kP2qoLPxDTpLq0v0edVtALQlwg3QAdhsNuUUV+q/h0r0Q26JduaWamduiYrKm+6qCA80qV/0ibExF0YFqXdEoMw+5/kAX6vVfhXJqeGkqqR+LMTZttePlXCVwcveJeQUXE4KL6Hd7YNFAbQLwg3Qzmw2+yXXO3NL9UNuif57qEQ7c0tU3ES3ktHLoD4RgRoQE6J+0fa5Yy6MClJ4oIfOut1wiWuTIeTUFpQmwkp1qeuXvDbF6GvvAjKH1F/REtJ4CY47pdWFf06BjoL/GoE2ZLXadKCovD7ElOqHnBLtPFSisiYmwPMxGtQ3MkgDY0M0IDZEA2NDdFFUkMzWSqn4gGQrk2ylUolVKrHZg4DNVv9lXv94xnVX9lczzteC/a2Ws7SilLR+ODnr0sR+njBxG3AeI9wArcRitSmz8LijW+mH3BLtOlSq49WNg4yv0UsXRQcpMTZEiTH2INM3KtA+b0x5kZT1jbTrG+njTVLeDvtEZOcTL5/Tt5icLZh4yqyyAFqMcAO0QJ3Fqp+OHHeMjdmZax/0W1HTOISYvL3UPybYEWIGxAarb2SQfIz1E2SV5EgHP5XSN0kHN0lHdjd+Q7+ukrdJksE+vsNgqL9q5uR1rzOs6yyvn7xuaMb5GtbVzPevP6eXt2QKdg4iTYUYbzNXBQFoMcINcBY1dVb9WFBWH2LsLTIZeaWqrmvcfeLnY9SAmGB7i0x919IF3QLk3RBkbDap6Ccp/T3p4DdS1ibpWFaj86jbRVL8aKnHaCl+lH1cBwCgWQg3wEmq6yzam2/vWmoY7Ls7r0w1lsZBJtDkrf4xwRoYG6LEWPtjQnig8yR4VouU/729m+ngJvtj+RHnExmMUvRge5iJHy11HykFhLXxJwUAz0W4wXmrqtaijLxS7TxUqp31A333Hi5TraXxpcDBZm9Ha4x9nEyweoYFyOvU2XzrqqXsbdLBjfYgk73ZfgXPybzNUuyw+jAzSoq7RDIFtuEnBYDzC+EG54WKmjpl5DVcrWQfJ/NjwXFZmrg1Qai/T31rzInBvt27+jU9EV51mZT9XX0X0zdSztbGNy80BUs9Rko9RtkDTczF9eNnAABtgXADj1RWVatPduZr074i7cwt0b4jx9XULZbCA30dISaxvnspNvQ0QUay35036xt7mDm40d7ldOqlywER9haZ+DH2QBM5wD4dOwCgXRBu4DHqLFZt+LFQ727PVequfFXVOoeOiCDTiRaZ+sG+kcGmM9+a4Fh2/ViZTfZAU7in8T6h8fYg0xBouvbiSh8AcCPCDTo1m82mnbmlend7jv6145AKj5+4XcEF3QJ09cBoDekRqsSYEEUEn2XuE5tNKtx7YuDvwU32ewedKqL/iS6m+NFScEwrfyoAwLkg3KBTyj1Wqfe35+q97bn6qeC4Y3tYgK+uHRyjG4bGamBsyJlbZSx10uEf7CHm4CYp61upotB5H4NRihly4rLsHiMl/65t9KkAAK2BcINOo6yqVh//kK93t+fou8yjjjtjm7y99PP+kbphaKzG9el2YnK8U9VWSblpJ7qYsr+z38foZN5mKW74iVaZuOGSb0DbfjAAQKsi3KBDq7VYteHHI3p3W65Sdx12mjhvZK+uuuHiOP1iYJSCzT6ND648Zr96Kau+ZSY3TbKccpdtc4h9XpmGMBM9RPL2beNPBQBoS4QbdDgN42jWbbOPoykqdx5Hc8PQOE2+OFaxoX4nDqo+br9yKXebdGi7fTm6r/HJAyNPmvl3tH38jNdpWnoAAJ0S4QYdRrPH0dRVSfk7pT31IebQNunIHklNXOvdJeFEq0yPUVzJBADnAcIN3OrkcTTf7j/q2G7y9tKEAVG6cVA3jQk5Ip/876Rt26UPtksFGZK18Z22FRxrnyDv5IXBvwBw3iHcoN2dbhyNURbd0P24boop1BBjpnzz06V3dzYeJyNJAd2kmKH2ABM71D5WJiiynT8JAKAjItygXdhsNn2fU6L3tufqXzsO6Wh5lRIM+fqFYb/GB2drlDlLURV75XWkUjrlvpIyh54IMQ0tMsGxdC8BAJpEuEGbyimu0D+35+qbtDSFFP9XA7326yXDfg0yH1CgKuw71dQvkuQbZJ9XJmbIiZaZLj0JMgCAZiPcoHXZbCo7kqUd332ugt3fKrz0v7rdK1O/NRyXTr3C2ttPih50IsTEXCyF9ebqJQDAOSHc4NwcL5AObZclZ5uO/vSdTIfTFWwp1tiG1+vvF2nx8pEiE2WMPWmcTPiFkpE/QQBA6+KbBc1XcVTKS3fMJWM7lC5DaY4ke4bpVr9bnc1LB43xqo4YpJj+YxTa+xIZI/pL3ia3lQ4AOH8QbtC0qlIpb8eJCfEObZOKDzjtYpBktRm0zxaj7229tN+nr8L6jNCI0Zepf4+IM9/XCQCANkK4gV3FUenA19KBDfbHggw1NSlenjFGW2ri9b21l7639tI+714a0z9B1w+N1XW9w+V9uvs6AQDQTgg356vKYvv9ljLrw8zhnTo1zNhC4lQQNEDfVPbQ+wUR2lYbr1IFymCQRvUK000Xx+oXiVEKauq+TgAAuAnh5nxRVWK/E/aBDfYl73s1apnpdpHUc5zyug7T6twY/T2jWkcPn5hAr29koH5zcZwmXxyj6BA/AQDQERFuPFV1mZT1rT3IZG6wDwS2WZ33CesjJYyTeo6Teo6VAiO0LatYt7/6rapqyyRJ4YEmXTckRtdfHKsBMcGMowEAdHiEG09RU14fZurHzeRuk2wW53269qoPMvVhJjja6eV9R47rnlVbVFVr1fCeXfTby3trLONoAACdDOGms6qtlLI3n2iZyU2TrLXO+4TGO7fMhMSd9nQFpVWamrJZxRW1GhQXolV3X6IAE38eAIDOh2+vzqK2SsrdWj8AeIOUs6XxDSWD406EmYRxUmiPZp26rKpW01ZuUU5xpXqG+Stl2nCCDQCg0+IbrKOqq7G3xhzYIGV+ZQ8zdVXO+wRFnwgyPce16B5MNXVWzXgzTbvyShUe6KvXp1+i8EAm2wMAdF6Em47CUmufLC/zK3ugyfpOqqt03icw0t691HOclDDePobmHAb4Wq02PfzODm38qUj+vkalTBuu+LCAc/wgAAC4F+HGXSx19hmAD3xl72rK+laqLXfexz/cHmYSxkk9x0vhfVr17tjPfrJb/0w/JG8vg5bdmaRBcaGtdm4AANyFcNNerBYp//sTY2YOfiPVlDnv49elvmVmvD3QdLuoVcPMyV7bsF/Lv9ovSZp/4yBd2rfbWY4AAKBzINy0FavVPutvw+0MDm60T6R3MnOIFD/2xJiZiP6SV9tfdr1+xyE9/WGGJOmRX1yoG5NOfxUVAACdDeGmtVit0pGMk1pmNtpvcXAyU7AUP/rEIODIRMnL2K5lbvqpUA/9I12SNHVUvH5z6QXt+v4AALQ1wk1ryd8hLb/MeZtvoNRjVH3LzFgparBkdN+PfNehUv16dZpqLTZNGhilJ68dwIzDAACPQ7hpLVGD7JdmR/Q7MW4mZohk7Bg3lcw+WqFpKzfreHWdRiR01aKbh8joRbABAHgewk1r8TJKD+5qlzEzriour9HUlZtVUFatCyODtHzKMJl92rc7DACA9tLxvok7sw4YbCprLLrn9S3af6Rc0SFmrZo+XCF+HaM1CQCAttDxvo3RauosVv3ure3alnVMwWZvvT79EkWH+Lm7LAAA2hThxkPZbDY98c+d+nfGYfl6e2nFtOHqGxnk7rIAAGhzhBsP9cJnP+qtzdkyGKQXbx2i4T27urskAADaBeHGA721OUuL//2jJGneLwfoF4nRbq4IAID2Q7jxMP/edViPv/eDJOmBy3vrrlE93VsQAADtjHDjQbZlFeuBt7bJapNuSorTQxP6urskAADaHeHGQ+w7clz3rNqiqlqrLruwm/5yw0BmHwYAnJcINx6goLRKU1ZsVnFFrQbHhWjpHUPlY+RXCwA4P/EN2MmVVtVq6sotyj1WqZ5h/kqZNlz+vkw8DQA4fxFuOrHqOotmrE5TRl6pwgN99cb0EQoLNLm7LAAA3Ipw00lZrTbNeft7bdpXpABfo1ZOu0Q9wvzdXRYAAG5HuOmk/vJRhv6145C8vQxadmeSBsaFuLskAAA6BMJNJ/Tahv167etMSdKC/xmk8X27ubkiAAA6DsJNJ/PP9Fw9/WGGJOmxiRfphqFxbq4IAICOhXDTiWz8qVBz3t4hSZo2uqfuG9/LzRUBANDxEG46if8eKtF9q9NUa7Hp6oHRevKa/kzSBwBAEwg3nUD20QpNW7lFx6vrNCKhqxbePFheXgQbAACa4vZws3TpUiUkJMhsNispKUkbNmw44/5r1qzR4MGD5e/vr+joaN19990qKipqp2rb39HyGk1N2awjZdW6KCpIy6cMk9nH6O6yAADosNwabtauXatZs2bp8ccf1/bt2zVu3DhNnDhRWVlZTe7/9ddfa8qUKbrnnnv03//+V2+//ba2bNmie++9t50rbx+VNRbd8/oW7S8sV0yIWavuvkQhfj7uLgsAgA7NreFm0aJFuueee3TvvfeqX79+Wrx4sbp3765ly5Y1uf+3336rnj17aubMmUpISNDYsWN13333aevWre1cedurs1j1wN+2aXvWMYX4+ej16ZcoKsTs7rIAAOjw3BZuampqlJaWpgkTJjhtnzBhgjZt2tTkMaNHj1ZOTo4++ugj2Ww2HT58WO+8846uvvrq075PdXW1SktLnZaOzmaz6Q/v79Rnuwtk8vbSiqnD1CcyyN1lAQDQKbgt3BQWFspisSgyMtJpe2RkpPLz85s8ZvTo0VqzZo1uueUW+fr6KioqSqGhoXrppZdO+z7JyckKCQlxLN27d2/Vz9EWFv/7R/19S7a8DNKLt12sYT27urskAAA6DbcPKD71cmabzXbaS5x37dqlmTNn6sknn1RaWpo++eQTZWZmasaMGac9/9y5c1VSUuJYsrOzW7X+1va377L0wmc/SpLmXZeoqwZEubkiAAA6F293vXF4eLiMRmOjVpqCgoJGrTkNkpOTNWbMGD388MOSpEGDBikgIEDjxo3T008/rejo6EbHmEwmmUyd407ZqbsO6w/v/yBJ+t0VvXXnyHg3VwQAQOfjtpYbX19fJSUlKTU11Wl7amqqRo8e3eQxFRUV8vJyLtlotF8WbbPZ2qbQdpJ28Kge+Ns2WW3SzcPiNPvnfd1dEgAAnZJbu6Vmz56t1157TSkpKcrIyNCDDz6orKwsRzfT3LlzNWXKFMf+1157rd59910tW7ZM+/fv18aNGzVz5kxdcskliomJcdfHOGc/FRzXPa9vVXWdVZdf2E3PXD+Q2YcBAGght3VLSdItt9yioqIizZs3T3l5eUpMTNRHH32k+Hh7d0xeXp7TnDfTpk1TWVmZlixZooceejthgTYAACAASURBVEihoaG64oorNH/+fHd9hHN2uLRKU1M261hFrQZ3D9Vf7xgqH6Pbh0IBANBpGWydvT/HRaWlpQoJCVFJSYmCg4PdW0tVrW5++Rvtzi9TQniA3pkxSmGBnWN8EAAA7cmV72+XmwgyMzNbXBhOqK6z6L430rQ7v0zhgSa9fvclBBsAAFqBy+Gmd+/euvzyy/Xmm2+qqqqqLWryeFarTQ/9Y4e+2V+kAF+jVt09XD3C/N1dFgAAHsHlcLNjxw5dfPHFeuihhxQVFaX77rtPmzdvbovaPNYzH2Xog+/z5O1l0Mt3JSkxNsTdJQEA4DFcDjeJiYlatGiRcnNztXLlSuXn52vs2LEaMGCAFi1apCNHjrRFnR7j1a/2a8XX9q69/71psMb16ebmigAA8CwtvizH29tb119/vf7xj39o/vz52rdvn+bMmaO4uDhNmTJFeXl5rVmnR3h/e66e+ShDkjR34kWafHGsmysCAMDztDjcbN26Vffff7+io6O1aNEizZkzR/v27dPnn3+u3NxcXXfdda1ZZ6f39Y+FevidHZKku8f01K/H93JzRQAAeCaX57lZtGiRVq5cqT179mjSpEl64403NGnSJMfMwQkJCXrllVd00UUXtXqxndXO3BLdt3qrai02XT0oWk9c3Z9J+gAAaCMuh5tly5Zp+vTpuvvuuxUV1fRNHXv06KEVK1acc3GeIPtohaat3KLyGotG9uqqRTcPlpcXwQYAgLbCJH5t6Gh5jW5ctkmZheW6KCpI/5gxSsFmnzZ9TwAAPFGbTuK3cuVKvf322422v/3223r99dddPZ3Hqqip0/RVW5RZWK7YUD+9Pv0Sgg0AAO3A5XDz7LPPKjw8vNH2iIgI/eUvf2mVojq7OotVv/vbdqVnH1OIn49enz5ckcFmd5cFAMB5weVwc/DgQSUkJDTaHh8f73STy/OVzWbT4+/t1Ge7C2Ty9lLKtGHqHRHk7rIAADhvuBxuIiIi9P333zfavmPHDoWFhbVKUZ3Z86l7tXZrtrwM0ku3Xayk+K7uLgkAgPOKy+Hm1ltv1cyZM/XFF1/IYrHIYrHo888/1+9//3vdeuutbVFjp7Hmu4N68fOfJEl/npyoCQOavpoMAAC0HZcvBX/66ad18OBBXXnllfL2th9utVo1ZcqU83rMzTf7ivTE+zslSTOv6K07RsS7uSIAAM5PLb4UfO/evdqxY4f8/Pw0cOBAxcd3ji/ztroUvKrWotn/SFeQyUfP3jiQSfoAAGhFrnx/M89NK7JYbbLZbPI2tviuFgAAoAmufH+73C0lSTk5OVq/fr2ysrJUU1Pj9NqiRYtackqPYPQySKLFBgAAd3I53Hz22Wf65S9/qYSEBO3Zs0eJiYk6cOCAbDabhg4d2hY1AgAANJvL/Sdz587VQw89pJ07d8psNmvdunXKzs7WpZdeqptuuqktagQAAGg2l8NNRkaGpk6dKkny9vZWZWWlAgMDNW/ePM2fP7/VCwQAAHCFy+EmICBA1dXVkqSYmBjt27fP8VphYWHrVQYAANACLo+5GTlypDZu3Kj+/fvr6quv1kMPPaQffvhB7777rkaOHNkWNQIAADSby+Fm0aJFOn78uCTpT3/6k44fP661a9eqd+/eev7551u9QAAAAFe4FG4sFouys7M1aNAgSZK/v7+WLl3aJoUBAAC0hEtjboxGo6666iodO3asreoBAAA4Jy4PKB44cKD279/fFrUAAACcM5fDzTPPPKM5c+bogw8+UF5enkpLS50WAAAAd3L53lJeXify0Mk3h7TZbDIYDLJYLK1XXRtoy3tLAQCAttGm95b64osvWlwYAABAW3M53Fx66aVtUQcAAECrcDncfPXVV2d8ffz48S0uBgAA4Fy5HG4uu+yyRttOHnvT0cfcAAAAz+by1VLFxcVOS0FBgT755BMNHz5cn376aVvUCAAA0Gwut9yEhIQ02vbzn/9cJpNJDz74oNLS0lqlMAAAgJZwueXmdLp166Y9e/a01ukAAABaxOWWm++//95p3WazKS8vT88++6wGDx7caoUBAAC0hMvhZsiQITIYDDp17r+RI0cqJSWl1QoDAABoCZfDTWZmptO6l5eXunXrJrPZ3GpFAQAAtJTL4SY+Pr4t6gAAAGgVLg8onjlzpl588cVG25csWaJZs2a1SlEAAAAt5XK4WbduncaMGdNo++jRo/XOO++0SlEAAAAt5XK4KSoqanKum+DgYBUWFrZKUQAAAC3lcrjp3bu3Pvnkk0bbP/74Y/Xq1atVigIAAGgplwcUz549Ww888ICOHDmiK664QpL02WefaeHChVq8eHGrFwgAAOAKl8PN9OnTVV1drWeeeUZ//vOfJUk9e/bUsmXLNGXKlFYvEAAAwBUG26mz8bngyJEj8vPzU2BgYGvW1KZKS0sVEhKikpISBQcHu7scAADQDK58f7doEr+6ujr16dNH3bp1c2z/8ccf5ePjo549e7pcMAAAQGtxeUDxtGnTtGnTpkbbv/vuO02bNq01agIAAGgxl8PN9u3bm5znZuTIkUpPT2+VogAAAFrK5XBjMBhUVlbWaHtJSYksFkurFAUAANBSLoebcePGKTk52SnIWCwWJScna+zYsa1aHAAAgKtcHlC8YMECjR8/XhdeeKHGjRsnSdqwYYNKSkr0xRdftHqBAAAArnC55aZ///76/vvvdfPNN6ugoEBlZWWaMmWK9u7dq7q6uraoEQAAoNnOaZ4bSTp27JjWrFmjlJQUpaend/hxN8xzAwBA5+PK97fLLTcNPv/8c915552KiYnRkiVLNHHiRG3durWlpwMAAGgVLo25ycnJ0apVq5SSkqLy8nLdfPPNqq2t1bp169S/f/+2qhEAAKDZmt1yM2nSJPXv31+7du3SSy+9pEOHDumll15qy9oAAABc1uyWm08//VQzZ87Ub37zG/Xp06ctawIAAGixZrfcbNiwQWVlZRo2bJhGjBihJUuW6MiRI21ZGwAAgMuaHW5GjRqlV199VXl5ebrvvvv097//XbGxsbJarUpNTW1y1mIAAID2dk6Xgu/Zs0crVqzQ6tWrdezYMf385z/X+vXrW7O+Vsel4AAAdD7tcim4JF144YVasGCBcnJy9NZbb53LqQAAAFrFOYWbBkajUZMnT25Rq83SpUuVkJAgs9mspKQkbdiw4bT7Tps2TQaDodEyYMCAcykfAAB4kFYJNy21du1azZo1S48//ri2b9+ucePGaeLEicrKympy/xdeeEF5eXmOJTs7W127dtVNN93UzpUDAICO6pxvv3AuRowYoaFDh2rZsmWObf369dPkyZOVnJx81uPff/993XDDDcrMzFR8fHyz3pMxNwAAdD7tNubmXNTU1CgtLU0TJkxw2j5hwgRt2rSpWedYsWKFfvazn50x2FRXV6u0tNRpAQAAnstt4aawsFAWi0WRkZFO2yMjI5Wfn3/W4/Py8vTxxx/r3nvvPeN+ycnJCgkJcSzdu3c/p7oBAEDH5tYxN5JkMBic1m02W6NtTVm1apVCQ0M1efLkM+43d+5clZSUOJbs7OxzqhcAAHRsLt04szWFh4fLaDQ2aqUpKCho1JpzKpvNppSUFN11113y9fU9474mk0kmk+mc6wUAAJ2D21pufH19lZSUpNTUVKftqampGj169BmP/fLLL/XTTz/pnnvuacsSAQBAJ+S2lhtJmj17tu666y4NGzZMo0aN0vLly5WVlaUZM2ZIsncp5ebm6o033nA6bsWKFRoxYoQSExPdUTYAAOjA3BpubrnlFhUVFWnevHnKy8tTYmKiPvroI8fVT3l5eY3mvCkpKdG6dev0wgsvuKNkAADQwbl1nht3YJ4bAAA6n04xzw0AAEBbINwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKIQbAADgUQg3AADAoxBuAACARyHcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAA8CiEGwAA4FEINwAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKIQbAADgUQg3AADAoxBuAACARyHcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAA8CiEGwAA4FEINwAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKIQbAADgUQg3AADAoxBuAACARyHcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAA8CiEGwAA4FHcHm6WLl2qhIQEmc1mJSUlacOGDWfcv7q6Wo8//rji4+NlMpl0wQUXKCUlpZ2qBQAAHZ23O9987dq1mjVrlpYuXaoxY8bolVde0cSJE7Vr1y716NGjyWNuvvlmHT58WCtWrFDv3r1VUFCgurq6dq4cAAB0VAabzWZz15uPGDFCQ4cO1bJlyxzb+vXrp8mTJys5ObnR/p988oluvfVW7d+/X127dm3Re5aWliokJEQlJSUKDg5uce0AAKD9uPL97bZuqZqaGqWlpWnChAlO2ydMmKBNmzY1ecz69es1bNgwLViwQLGxserbt6/mzJmjysrK075PdXW1SktLnRYAAOC53NYtVVhYKIvFosjISKftkZGRys/Pb/KY/fv36+uvv5bZbNZ7772nwsJC3X///Tp69Ohpx90kJyfrqaeeavX6AQBAx+T2AcUGg8Fp3WazNdrWwGq1ymAwaM2aNbrkkks0adIkLVq0SKtWrTpt683cuXNVUlLiWLKzs1v9MwAAgI7DbS034eHhMhqNjVppCgoKGrXmNIiOjlZsbKxCQkIc2/r16yebzaacnBz16dOn0TEmk0kmk6l1iwcAAB2W21pufH19lZSUpNTUVKftqampGj16dJPHjBkzRocOHdLx48cd2/bu3SsvLy/FxcW1ab0AAKBzcGu31OzZs/Xaa68pJSVFGRkZevDBB5WVlaUZM2ZIsncpTZkyxbH/7bffrrCwMN19993atWuXvvrqKz388MOaPn26/Pz83PUxAABAB+LWeW5uueUWFRUVad68ecrLy1NiYqI++ugjxcfHS5Ly8vKUlZXl2D8wMFCpqan63e9+p2HDhiksLEw333yznn76aXd9BAAA0MG4dZ4bd2CeGwAAOp9OMc8NAABAWyDcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAA8CiEGwAA4FEINwAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUb3cXAADwfDabTXV1dbJYLO4uBR2Yj4+PjEbjOZ+HcAMAaFM1NTXKy8tTRUWFu0tBB2cwGBQXF6fAwMBzOg/hBgDQZqxWqzIzM2U0GhUTEyNfX18ZDAZ3l4UOyGaz6ciRI8rJyVGfPn3OqQWHcAMAaDM1NTWyWq3q3r27/P393V0OOrhu3brpwIEDqq2tPadww4BiAECb8/Li6wZn11qtevy1AQAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAAnUBtba27S+g0CDcAgHZjs9lUUVPnlsVms7lU6yeffKKxY8cqNDRUYWFhuuaaa7Rv3z7H6zk5Obr11lvVtWtXBQQEaNiwYfruu+8cr69fv17Dhg2T2WxWeHi4brjhBsdrBoNB77//vtP7hYaGatWqVZKkAwcOyGAw6B//+Icuu+wymc1mvfnmmyoqKtJtt92muLg4+fv7a+DAgXrrrbeczmO1WjV//nz17t1bJpNJPXr00DPPPCNJuuKKK/TAAw847V9UVCSTyaTPP//cpZ9PR8Y8NwCAdlNZa1H/J//PLe+9a95V8vdt/tdeeXm5Zs+erYEDB6q8vFxPPvmkrr/+eqWnp6uiokKXXnqpYmNjtX79ekVFRWnbtm2yWq2SpA8//FA33HCDHn/8ca1evVo1NTX68MMPXa750Ucf1cKFC7Vy5UqZTCZVVVUpKSlJjz76qIKDg/Xhhx/qrrvuUq9evTRixAhJ0ty5c/Xqq6/q+eef19ixY5WXl6fdu3dLku6991498MADWrhwoUwmkyRpzZo1iomJ0eWXX+5yfR0V4QYAgCbceOONTusrVqxQRESEdu3apU2bNunIkSPasmWLunbtKknq3bu3Y99nnnlGt956q5566inHtsGDB7tcw6xZs5xafCRpzpw5jue/+93v9Mknn+jtt9/WiBEjVFZWphdeeEFLlizR1KlTJUkXXHCBxo4d6/hMv/vd7/TPf/5TN998syRp5cqVmjZtmkfNHE24AQC0Gz8fo3bNu8pt7+2Kffv26YknntC3336rwsJCR6tMVlaW0tPTdfHFFzuCzanS09P1q1/96pxrHjZsmNO6xWLRs88+q7Vr1yo3N1fV1dWqrq5WQECAJCkjI0PV1dW68sormzyfyWTSnXfeqZSUFN18881KT0/Xjh07GnWRdXaEGwBAuzEYDC51DbnTtddeq+7du+vVV19VTEyMrFarEhMTVVNTIz8/vzMee7bXDQZDozFATQ0YbggtDRYuXKjnn39eixcv1sCBAxUQEKBZs2appqamWe8r2bumhgwZopycHKWkpOjKK69UfHz8WY/rTBhQDADAKYqKipSRkaE//OEPuvLKK9WvXz8VFxc7Xh80aJDS09N19OjRJo8fNGiQPvvss9Oev1u3bsrLy3Os//jjj826a/qGDRt03XXX6c4779TgwYPVq1cv/fjjj47X+/TpIz8/vzO+98CBAzVs2DC9+uqr+tvf/qbp06ef9X07G8INAACn6NKli8LCwrR8+XL99NNP+vzzzzV79mzH67fddpuioqI0efJkbdy4Ufv379e6dev0zTffSJL++Mc/6q233tIf//hHZWRk6IcfftCCBQscx19xxRVasmSJtm3bpq1bt2rGjBny8fE5a129e/dWamqqNm3apIyMDN13333Kz893vG42m/Xoo4/qkUce0RtvvKF9+/bp22+/1YoVK5zOc++99+rZZ5+VxWLR9ddff64/rg6HcAMAwCm8vLz097//XWlpaUpMTNSDDz6o5557zvG6r6+vPv30U0VERGjSpEkaOHCgnn32WcedrC+77DK9/fbbWr9+vYYMGaIrrrjC6TLxhQsXqnv37ho/frxuv/12zZkzp1l3TX/iiSc0dOhQXXXVVbrsssscAevUfR566CE9+eST6tevn2655RYVFBQ47XPbbbfJ29tbt99+u8xm87n8qDokg83VC/87udLSUoWEhKikpETBwcHuLgcAPFpVVZUyMzOVkJDgkV+inVV2drZ69uypLVu2aOjQoe4ux+FMfy+ufH93jlFdAADgnNXW1iovL0+PPfaYRo4c2aGCTWuiWwoAgPPExo0bFR8fr7S0NL388svuLqfN0HIDAMB54rLLLnP5NhSdES03AADAoxBuAACARyHcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAAbaBnz55avHixu8s4LxFuAACARyHcAAAAJxaLRVar1d1ltBjhBgDQfmw2qabcPYsLM/O+8sorio2NbfQF/8tf/lJTp07Vvn37dN111ykyMlKBgYEaPny4/v3vf7f4x7Jo0SINHDhQAQEB6t69u+6//34dP37caZ+NGzfq0ksvlb+/v7p06aKrrrpKxcXFkiSr1ar58+erd+/eMplM6tGjh5555hlJ0n/+8x8ZDAYdO3bMca709HQZDAYdOHBAkrRq1SqFhobqgw8+UP/+/WUymXTw4EFt2bJFP//5zxUeHq6QkBBdeuml2rZtm1Ndx44d069//WtFRkbKbDYrMTFRH3zwgcrLyxUcHKx33nnHaf9//etfCggIUFlZWYt/XmfD7RcAAO2ntkL6S4x73vv/HZJ8A5q160033aSZM2fqiy++0JVXXilJKi4u1v/93//pX//6l44fP65Jkybp6aefltls1uuvv65rr71We/bsUY8ePVwuzcvLSy+++KJ69uypzMxM3X///XrkkUe0dOlSSfYwcuWVV2r69Ol68cUX5e3trS+++EIWi0WSNHfuXL366qt6/vnnNXbsWOXl5Wn37t0u1VBRUaHk5GS99tprCgsLU0REhDIzMzV16lS9+OKLkqSFCxdq0qRJ+vHHHxUUFCSr1aqJEyeqrKxMb775pi644ALt2rVLRqNRAQEBuvXWW7Vy5Ur9z//8j+N9GtaDgoJc/jk1F+EGAIBTdO3aVb/4xS/0t7/9zRFu3n77bXXt2lVXXnmljEajBg8e7Nj/6aef1nvvvaf169frgQcecPn9Zs2a5XiekJCgP//5z/rNb37jCDcLFizQsGHDHOuSNGDAAElSWVmZXnjhBS1ZskRTp06VJF1wwQUaO3asSzXU1tZq6dKlTp/riiuucNrnlVdeUZcuXfTll1/qmmuu0b///W9t3rxZGRkZ6tu3rySpV69ejv3vvfdejR49WocOHVJMTIwKCwv1wQcfKDU11aXaXEW4AQC0Hx9/ewuKu97bBXfccYd+/etfa+nSpTKZTFqzZo1uvfVWGY1GlZeX66mnntIHH3ygQ4cOqa6uTpWVlcrKympRaV988YX+8pe/aNeuXSotLVVdXZ2qqqpUXl6ugIAApaen66abbmry2IyMDFVXVztCWEv5+vpq0KBBTtsKCgr05JNP6vPPP9fhw4dlsVhUUVHh+Jzp6emKi4tzBJtTXXLJJRowYIDeeOMNPfbYY1q9erV69Oih8ePHn1OtZ8OYGwBA+zEY7F1D7lgMBpdKvfbaa2W1WvXhhx8qOztbGzZs0J133ilJevjhh7Vu3To988wz2rBhg9LT0zVw4EDV1NS4/CM5ePCgJk2apMTERK1bt05paWn661//KsnemiJJfn5+pz3+TK9J9i4vSU53A28476nnMZzyM5o2bZrS0tK0ePFibdq0Senp6QoLC3N8zrO9t2RvvVm5cqUke5fU3Xff3eh9WhvhBgCAJvj5+emGG27QmjVr9NZbb6lv375KSkqSJG3YsEHTpk3T9ddfr4EDByoqKsoxONdVW7duVV1dnRYuXKiRI0eqb9++OnTIuXVr0KBB+uyzz5o8vk+fPvLz8zvt6926dZMk5eXlObalp6c3q7YNGzZo5syZmjRpkgYMGCCTyaTCwkKnunJycrR3797TnuPOO+9UVlaWXnzxRf33v/91dJ21JcINAACncccdd+jDDz9USkqKo9VGknr37q13331X6enp2rFjh26//fYWXzp9wQUXqK6uTi+99JL279+v1atX6+WXX3baZ+7cudqyZYvuv/9+ff/999q9e7eWLVumwsJCmc1mPfroo3rkkUf0xhtvaN++ffr222+1YsUKR63du3fXn/70J+3du1cffvihFi5c2KzaevfurdWrVysjI0Pfffed7rjjDqfWmksvvVTjx4/XjTfeqNTUVGVmZurjjz/WJ5984tinS5cuuuGGG/Twww9rwoQJiouLa9HPyRWEGwAATuOKK65Q165dtWfPHt1+++2O7c8//7y6dOmi0aNH69prr9VVV12loUOHtug9hgwZokWLFmn+/PlKTEzUmjVrlJyc7LRP37599emnn2rHjh265JJLNGrUKP3zn/+Ut7d96OwTTzyhhx56SE8++aT69eunW265RQUFBZIkHx8fvfXWW9q9e7cGDx6s+fPn6+mnn25WbSkpKSouLtbFF1+su+66SzNnzlRERITTPuvWrdPw4cN12223qX///nrkkUccV3E1uOeee1RTU6Pp06e36GfkKoPN5sKF/x6gtLRUISEhKikpUXBwsLvLAQCPVlVVpczMTCUkJMhsNru7HLjJmjVr9Pvf/16HDh2Sr6/vafc709+LK9/fXC0FAADaREVFhTIzM5WcnKz77rvvjMGmNdEtBQBAG1qzZo0CAwObXBrmqvFUCxYs0JAhQxQZGam5c+e22/vSLQUAaDN0S9kn2Tt8+HCTr/n4+Cg+Pr6dK+q46JYCAKATCAoKatNbDaAxuqUAAG3uPOskQAu11t8J4QYA0GZ8fHwk2QeWAmfTMPOx0Wg8p/PQLQUAaDNGo1GhoaGOOVf8/f3bfOp9dE5Wq1VHjhyRv7+/Y/6eliLcAADaVFRUlCQ5Ag5wOl5eXurRo8c5B2DCDQCgTRkMBkVHRysiIqLJGzYCDXx9fR03+jwXhBsAQLswGo3nPJYCaA63DyheunSp43r2pKQkbdiw4bT7/uc//5HBYGi07N69ux0rBgAAHZlbw83atWs1a9YsPf7449q+fbvGjRuniRMnKisr64zH7dmzR3l5eY6lT58+7VQxAADo6NwabhYtWqR77rlH9957r/r166fFixere/fuWrZs2RmPi4iIUFRUlGOhmRMAADRw25ibmpoapaWl6bHHHnPaPmHCBG3atOmMx1588cWqqqpS//799Yc//EGXX375afetrq5WdXW1Y72kpESSfRpnAADQOTR8bzdnoj+3hZvCwkJZLBZFRkY6bY+MjFR+fn6Tx0RHR2v58uVKSkpSdXW1Vq9erSuvvFL/+c9/NH78+CaPSU5O1lNPPdVoe/fu3c/9QwAAgHZVVlamkJCQM+7j9qulTr2W3Waznfb69gsvvFAXXnihY33UqFHKzs7W//7v/5423MydO1ezZ892rFutVh09elRhYWGtPpFUaWmpunfvruzsbG7K2QHw++hY+H10PPxOOhZ+H2dms9lUVlammJiYs+7rtnATHh4uo9HYqJWmoKCgUWvOmYwcOVJvvvnmaV83mUwymUxO20JDQ10r1kXBwcH8YXYg/D46Fn4fHQ+/k46F38fpna3FpoHbBhT7+voqKSlJqampTttTU1M1evToZp9n+/btio6OkNJq+AAACTBJREFUbu3yAABAJ+XWbqnZs2frrrvu0rBhwzRq1CgtX75cWVlZmjFjhiR7l1Jubq7eeOMNSdLixYvVs2dPDRgwQDU1NXrzzTe1bt06rVu3zp0fAwAAdCBuDTe33HKLioqKNG/ePOXl5SkxMVEfffSR4uPjJUl5eXlOc97U1NRozpw5ys3NlZ+fnwYMGKAPP/xQkyZNctdHcGIymfTHP/6xUTcY3IPfR8fC76Pj4XfSsfD7aD0GW3OuqQIAAOgk3H77BQAAgNZEuAEAAB6FcAMAADwK4QYAAHgUwk0rWbp0qRISEmQ2m5WUlKQNGza4u6TzVnJysoYPH66goCBFRERo8uTJ2rNnj7vLQr3k5GQZDAbNmjXL3aWct3Jzc3XnnXcqLCxM/v7+GjJkiNLS0txd1nmprq5Of/jDH5SQkCA/Pz/16tVL8+bNk9VqdXdpnRrhphWsXbtWs2bN0uOPP67t27dr3LhxmjhxotNl7Gg/X375pX7729/q22+/VWpqqurq6jRhwgSVl5e7u7Tz3pYtW7R8+XINGjTI3aWct4qLizVmzBj5+Pjo448/1q5du7Rw4cI2n7kdTZs/f75efvllLVmyRBkZGVqwYIGee+45vfTSS+4urVPjUvBWMGLECA0dOlTLli1zbOvXr58mT56s5ORkN1YGSTpy5IgiIiL05ZdfnvYeZGh7x48f19ChQ7V06VI9/fTTGjJkiBYvXuzuss47jz32mDZu3EjrcgdxzTXXKDIyUitWrHBsu/HGG+Xv76/Vq1e7sbLOjZabc1RTU6O0tDRNmDDBafuECRO0adMmN1WFk5WUlEiSunbt6uZKzm+//e1vdfXVV+tnP/uZu0s5r61fv17D/n979xfSZNuAAfx6XDm3MWI51ElYRn9sWmEuYjqI8kSDoLAkWbboQCy3VqMossgC7cxOisGiPEkxBv1ZRH/MIFEIpZqOsDwIKghZElRL2sF2vwe9DMb63u97v2yPe3b94IHtfrZ5PUde3M+93RYLdu/ejYKCAlRWVuLKlStyx8paNpsNg4ODmJqaAgCMj49jeHh43vw4baaSfVfwTDczM4NYLJay2WdhYWHKpqCUfkIIeDwe2Gw2VFRUyB0na/X39+PFixcYGxuTO0rWe/v2LbxeLzweD06dOoXR0VEcPnwYarUa+/btkzte1jlx4gS+fPmCsrIyqFQqxGIxdHZ2oqmpSe5oGY3lZo5IkpT0XAiRMkbp53Q6MTExgeHhYbmjZK0PHz7A7Xbj0aNHyMvLkztO1ovH47BYLOjq6gIAVFZW4tWrV/B6vSw3Mrhx4wauX7+Ovr4+lJeXIxgM4siRIyguLobD4ZA7XsZiuflNRqMRKpUqZZYmHA6nzOZQerlcLgQCAQwNDWHJkiVyx8laz58/RzgcRlVVVWIsFothaGgIly5dQjQahUqlkjFhdjGZTDCbzUlja9as4QbEMjl+/DhOnjyJPXv2AADWrl2Ld+/e4cKFCyw3v4Frbn5Tbm4uqqqqMDAwkDQ+MDCA6upqmVJlNyEEnE4nbt68iSdPnqC0tFTuSFmttrYWoVAIwWAwcVgsFtjtdgSDQRabNKupqUn5aYSpqanEhsWUXrOzs8jJSf5XrFKp+FXw38SZmzng8XjQ3NwMi8UCq9UKn8+H9+/fo7W1Ve5oWamtrQ19fX24c+cO9Hp9YlZt0aJF0Gg0MqfLPnq9PmW9k06nQ35+PtdByeDo0aOorq5GV1cXGhsbMTo6Cp/PB5/PJ3e0rLR9+3Z0dnaipKQE5eXlePnyJbq7u3HgwAG5o2U2QXPi8uXLYunSpSI3N1ds2LBBPH36VO5IWQvAL4+enh65o9HfNm/eLNxut9wxstbdu3dFRUWFUKvVoqysTPh8PrkjZa2vX78Kt9stSkpKRF5enli+fLlob28X0WhU7mgZjb9zQ0RERIrCNTdERESkKCw3REREpCgsN0RERKQoLDdERESkKCw3REREpCgsN0RERKQoLDdERESkKCw3RET4ufnt7du35Y5BRHOA5YaIZLd//35IkpRy1NXVyR2NiDIQ95Yionmhrq4OPT09SWNqtVqmNESUyThzQ0TzglqtRlFRUdJhMBgA/Lxl5PV6UV9fD41Gg9LSUvj9/qT3h0IhbN26FRqNBvn5+WhpaUEkEkl6zbVr11BeXg61Wg2TyQSn05l0fmZmBjt37oRWq8XKlSsRCAT+7EUT0R/BckNEGeHMmTNoaGjA+Pg49u7di6amJkxOTgIAZmdnUVdXB4PBgLGxMfj9fjx+/DipvHi9XrS1taGlpQWhUAiBQAArVqxI+hvnzp1DY2MjJiYmsG3bNtjtdnz+/Dmt10lEc0DunTuJiBwOh1CpVEKn0yUd58+fF0L83Om9tbU16T2bNm0SBw8eFEII4fP5hMFgEJFIJHH+3r17IicnR0xPTwshhCguLhbt7e3/MQMAcfr06cTzSCQiJEkS9+/fn7PrJKL04JobIpoXtmzZAq/XmzS2ePHixGOr1Zp0zmq1IhgMAgAmJyexfv166HS6xPmamhrE43G8efMGkiTh48ePqK2t/ccM69atSzzW6XTQ6/UIh8P/9zURkTxYbohoXtDpdCm3if4bSZIAAEKIxONfvUaj0fxPn7dw4cKU98bj8X+ViYjkxzU3RJQRnj17lvK8rKwMAGA2mxEMBvH9+/fE+ZGREeTk5GDVqlXQ6/VYtmwZBgcH05qZiOTBmRsimhei0Simp6eTxhYsWACj0QgA8Pv9sFgssNls6O3txejoKK5evQoAsNvtOHv2LBwOBzo6OvDp0ye4XC40NzejsLAQANDR0YHW1lYUFBSgvr4e3759w8jICFwuV3ovlIj+OJYbIpoXHjx4AJPJlDS2evVqvH79GsDPbzL19/fj0KFDKCoqQm9vL8xmMwBAq9Xi4cOHcLvd2LhxI7RaLRoaGtDd3Z34LIfDgR8/fuDixYs4duwYjEYjdu3alb4LJKK0kYQQQu4QRET/RJIk3Lp1Czt27JA7ChFlAK65ISIiIkVhuSEiIiJF4ZobIpr3ePeciP4NztwQERGRorDcEBERkaKw3BAREZGisNwQERGRorDcEBERkaKw3BAREZGisNwQERGRorDcEBERkaKw3BAREZGi/AVjr3T/fhJQPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy on the training and validation datasets over training epochs\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, test_labels_cat, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on training is lower than validation accuracy at the firest epoch but since the second epoch it became higher. So our CNN is initially underfitting at the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model:\n",
    "\n",
    "Fine-tune the model's hyperparameters: You can also try adjusting the hyperparameters of the model, such as the learning rate, the batch size, and the number of epochs, to see if this improves the model's performance.\n",
    "\n",
    "Use data augmentation: Data augmentation involves generating additional training data by applying random transformations to the existing data. This can help the model learn more robust features and improve its generalization to unseen data.\n",
    "\n",
    "Add regularization: Regularization techniques, such as dropout and weight decay, can help prevent overfitting by adding constraints to the model's parameters.\n",
    "\n",
    "Use a deeper or more complex model: A deeper or more complex model may be able to learn more about the patterns in the data and achieve higher accuracy. \n",
    "\n",
    "Try a different optimization algorithm: Different optimization algorithms can have different effects on the model's performance. \n",
    "\n",
    "Experiment with different preprocessing techniques: Preprocessing the data, such as normalizing or standardizing the inputs, can have a significant impact on the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** Create an autoencoder network with the following properties:\n",
    "- the `encoder` consists of $3$ convolutional and $3$ max pooling layers used alternately\n",
    "    - convolutional layers consist of $64$, $32$ and $16$ kernels, respectively, of size $3\\times 3$, and with the $\\mathrm{ReLU}$ activation function \n",
    "    - each max pooling layer is with the pool size $2\\times 2$\n",
    "- the `decoder` consists of $3$ convolutional and $3$ up sampling layers used alternately\n",
    "    - convolutional layers consist of $16$, $32$ and $64$ kernels, respectively, of size $3\\times 3$, and with the $\\mathrm{ReLU}$ activation function at each layer, except at the last (output) layer where the *sigmoid* activation function is used\n",
    "    - each up sampling layer is with the usampling factors of size $2\\times 2$.\n",
    "\n",
    "Use the `Adam` optimizer and `MSE` error as a loss function. \n",
    "\n",
    "**Note:** Name the output of the final max pooling layer, i.e., the model that produces the latent representation, `encoder` (this will be useful for the second part of the assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set \n",
    "(X_train, _), (X_test, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "image_size = X_train.shape[1]\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (-1, image_size, image_size, 1))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "\n",
    "X_test = np.reshape(X_test, (-1, image_size, image_size, 1))\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(loc=0.5, scale=0.5, size=X_train.shape)\n",
    "X_train_noisy = X_train + noise\n",
    "\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=X_test.shape)\n",
    "X_test_noisy = X_test + noise\n",
    "\n",
    "# We will cut off the values that fell out of the range [0,1] after adding the noise\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_111 (Conv2D)         (None, 28, 28, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_51 (MaxPoolin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_112 (Conv2D)         (None, 14, 14, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_52 (MaxPoolin  (None, 7, 7, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_113 (Conv2D)         (None, 7, 7, 16)          4624      \n",
      "                                                                 \n",
      " max_pooling2d_53 (MaxPoolin  (None, 3, 3, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 144)               0         \n",
      "                                                                 \n",
      " latent_vector (Dense)       (None, 16)                2320      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,048\n",
      "Trainable params: 26,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the input shape\n",
    "input_shape = (28, 28, 1)\n",
    "latent_dim = 16\n",
    "\n",
    "# Define the number of kernels for each convolutional layer\n",
    "encoder_conv_kernels = [64, 32, 16]\n",
    "decoder_conv_kernels = [16, 32, 64]\n",
    "\n",
    "# Define the kernel size for all convolutional layers\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "# Define the pool size for all max pooling layers\n",
    "pool_size = (2, 2)\n",
    "\n",
    "# Define the up sampling factors for all up sampling layers\n",
    "up_sampling_factors = (2, 2)\n",
    "\n",
    "# Define the encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "\n",
    "for filters in encoder_conv_kernels:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "    x = MaxPool2D(pool_size=pool_size)(x)\n",
    "\n",
    "# Read and preserve the dimensions of the tensor\n",
    "shape = backend.int_shape(x)\n",
    "\n",
    "# Then we have a flattening layer\n",
    "x = Flatten()(x)\n",
    "\n",
    "latent_outputs = Dense(latent_dim, name='latent_vector')(x)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = Model(inputs=inputs, outputs=latent_outputs, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 3, 3, 16)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " decoder_input (InputLayer)                      [(None, 16)]                                0                \n",
      "                                                                                                              \n",
      " dense_11 (Dense)                                (None, 144)                                 2448             \n",
      "                                                                                                              \n",
      " reshape_12 (Reshape)                            (None, 3, 3, 16)                            0                \n",
      "                                                                                                              \n",
      " conv2d_114 (Conv2D)                             (None, 3, 3, 64)                            9280             \n",
      "                                                                                                              \n",
      " up_sampling2d_48 (UpSampling2D)                 (None, 6, 6, 64)                            0                \n",
      "                                                                                                              \n",
      " conv2d_115 (Conv2D)                             (None, 6, 6, 32)                            18464            \n",
      "                                                                                                              \n",
      " up_sampling2d_49 (UpSampling2D)                 (None, 12, 12, 32)                          0                \n",
      "                                                                                                              \n",
      " conv2d_116 (Conv2D)                             (None, 12, 12, 16)                          4624             \n",
      "                                                                                                              \n",
      " up_sampling2d_50 (UpSampling2D)                 (None, 24, 24, 16)                          0                \n",
      "                                                                                                              \n",
      " conv2d_117 (Conv2D)                             (None, 24, 24, 1)                           145              \n",
      "                                                                                                              \n",
      " decoder_output (Activation)                     (None, 24, 24, 1)                           0                \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 34,961\n",
      "Trainable params: 34,961\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the decoder model\n",
    "# First we have the input layer\n",
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "for filters in decoder_conv_kernels[::-1]:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "    x = UpSampling2D(size=up_sampling_factors)(x)\n",
    "\n",
    "# Define the output layer with sigmoid activation function\n",
    "# then we add one more convolutional layer to control the channel dimension   \n",
    "x = Conv2D(filters=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same')(x)\n",
    "\n",
    "# and one activation later with the sigmoid activation function\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Define the decoder model\n",
    "decoder = Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "decoder.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3, 3, 16) dtype=float32 (created by layer 'reshape_9')>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (734568692.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [67], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    autoencoder = Model(inputs=inputs, outputs=decoder(encoder(inputs)), name='autoencoder'\u001b[0m\n\u001b[0m                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=inputs, outputs=decoder(encoder(inputs)), name='autoencoder'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = Conv2D(filters=1,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='sigmoid',\n",
    "                 padding='same',\n",
    "                 name='decoder_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the Adam optimizer and MSE loss function\n",
    "autoencoder.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Train the autoencoder over $10$ epochs. Set the `shuffle` parameter to True. Set the `batch_size` to $128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 16:57:35.364245: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2022-12-21 16:57:35.930559: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'autoencoder' (type Functional).\n    \n    Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 3, 3, 16), found shape=(None, 144)\n    \n    Call arguments received by layer 'autoencoder' (type Functional):\n      • inputs=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_noisy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filegdfjd6g2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/seoyangsam/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'autoencoder' (type Functional).\n    \n    Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 3, 3, 16), found shape=(None, 144)\n    \n    Call arguments received by layer 'autoencoder' (type Functional):\n      • inputs=tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_train_noisy,\n",
    "                X_train, epochs=10, shuffle=True, batch_size=128,\n",
    "                    validation_data=(X_test_noisy, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** Plot loss on the training and validation datasets over the training epochs. Add a legend to your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:** Show for an arbitrary image of the test set how the autoencoder works. In other words, plot an arbitrary original and  predicted image from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11:** Calculate the loss for the autoencoder on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the learned autoencoder for **similar image retrieval** task. More precisely, we can compute the distance between the latent-space vectors to look for similar images. Naturally, the smaller the distance, the more visually similar two images are. \n",
    "\n",
    "Your goal is to create a very simple image retrieval system by using the trained autoencoder and the [nearest-neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12:** \n",
    "    \n",
    "- Extract the encoder part of your autoencoder model and save it in the folder as `encoder.h5`.\n",
    "- Fit the encoder part on your test set and reshape the output in order to obtain the latent space vectors.\n",
    "- Choose an arbitrary query image from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13:** \n",
    "- Fit the NN algorithm to the encoded test set in order to obtain distances and indices of the data.\n",
    "- Find the $10$ closest images to the encoded query image.\n",
    "- Plot the obtained $10$ closest images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
